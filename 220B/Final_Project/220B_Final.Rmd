---
title: "220B_Final_Project"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggplot2)
library(GGally)
library(dplyr)
library(ggpubr)
```


```{r directorys, include =FALSE}

#Change working directory depending if on Mac vs Windows

tryCatch({
  setwd("/Users/owner/Desktop/PSTAT")},
  error = function(e) {
    print('Path for Mac working directory not found, trying Windows...')
    setwd("D:/PSTAT/")})
```


# Project Goal:

For this project, suppose a survey was conducted within San Francisco to investigate the number of individual consultations with pharmacists.

+ Investigate how the number of consultations with a pharmacist is associated with other
variables. As part of this investigation, develop a model for pharmacy managers to estimate
the expected number of pharmacist consultations by a new customer within a 4 week period, if
the pharmacy was provided with values of the other variables for that new customer.

+ For the first person in this dataset, use your fitted model to estimate the probability distribution for his/her number of pharmacy consultations within a 4 week period, i.e., estimate the probability that his/her number of pharmacist consultations equals 0,1,2, etc.

+ Clearly state and discuss your model(s) and assumptions, and also the limitations of your analysis in the context of this study. Within the discussion section, you may include any questions you would like to ask the people who designed the survey and collected these data

```{r Load in data}
pharmacy_data <- read.table("pharmacist.txt",header = T)

#inspect first few rows
head(pharmacy_data)

```

### We are trying to predict pc, which is the number of consultations with a pharmacist in the past 4 weeks.
+ pc variable represents counts, so we are modeling a Poisson distribution

## EDA

```{r Kernel Densities, warning=FALSE}
library(reshape2)

long_df=melt(pharmacy_data)
kernel_plots <- ggplot(long_df, aes(value, colour = variable)) + 
  geom_density(aes(fill=variable),alpha = 0.1)+
  facet_wrap(~variable, scales = "free") + 
  labs(color = 'Subject\nResponses') + guides(fill=F) + 
  theme(text = element_text(size = 12))

kernel_plots
```

Data either follows a bimodal, exponential, or skewed normal distribution. 


```{r Box Plots}
cov.boxs <- ggplot(stack(pharmacy_data), aes(x = ind, y = values, fill = ind)) +
  geom_boxplot(outlier.color = 'red', alpha = 0.4) + facet_wrap(~ind, scales = 'free')+
  theme(legend.position = "none", axis.title = element_blank(), text = element_text(size = 15))
cov.boxs
```

```{r Correlations}
library(Hmisc)

my_fn <- function(data, mapping, pts=list(), smt=list(), ...){
  ggplot(data = data, mapping = mapping, ...) + 
    do.call(geom_point, pts) +
    do.call(geom_smooth, smt) 
}

#p-values of correlations
rcorr(as.matrix(pharmacy_data), type = c('pearson'))

all_corr <- ggcorr(pharmacy_data, geom = "tile", label = T, hjust = .8, size = 5, color = "grey50")

# only plot the interesting correlations between pc, age, ad
p1 <- ggplot(pharmacy_data, aes(x = pc, y = age)) +
  geom_point() + geom_smooth(method = 'lm', se = F, size = 1.2, color = 'red', alpha = 0.5)+
  labs(title = 'PC vs Age') + theme_classic() + theme(plot.title = element_text(hjust = 0.5))

p2 <- ggplot(pharmacy_data, aes(x = pc, y = ad)) +
  geom_point() + geom_smooth(method = 'lm', se = F, size = 1.2, color = 'green', alpha = 0.5) + 
  labs(title = 'PC vs Ad') + theme_classic() +  theme(plot.title = element_text(hjust = 0.5))

p3 <- ggplot(pharmacy_data, aes(x = ad, y = age)) +
  geom_point() + geom_smooth(method = 'lm', se = F, size = 1.2, color = 'blue', alpha = 0.5) +
  labs(title = 'Ad vs Age') + theme_classic() +  theme(plot.title = element_text(hjust = 0.5))

int_corr <- ggarrange(all_corr,p1,p2,p3, nrow = 2, ncol = 2)


annotate_figure(int_corr, top =text_grob ("Correlations Between Survey Measures", face = "bold", size = 14))
```

Strongest correlations are between pc, ad, and age. These are the ones plotted. Everything else is around the +/- 0.3 range. Ad might be strongest predictor of pc

# Being Modeling

## Trying out a poisson model
```{r First pass}
fit1 <- glm(pc ~ ., family = 'poisson', data = pharmacy_data)

summary(fit1)

#check phi for over dispersion
rp <- residuals(fit1,type = 'pearson') #phi tilde numerator

phi_hat <- fit1$deviance/fit1$df.residual
phi_tilde <- sum(rp^2)/fit1$df.residual

fit1.phi <- as.data.frame(cbind('hat'=phi_hat,'tilde'=phi_tilde))
fit1.phi

#check for overdispersion using residuals
fv <- fitted(fit1)

par(mfrow = c(1,2))

plot(log(fv), rp, xlab = expression(log(hat(mu))),
     ylab = "Pearson Resids")
plot(log(fv), log(residuals(fit1, type = 'response')^2), xlab = expression(log(hat(mu))),
     ylab = expression(log(y - hat(mu))^2))
abline(0,1)
```
First pass of the model indicates that sex, age, ill, ad, hs, and ch1 are all significant predictors. Deviance diagnostics suggest that poisson is the true random component (phi hat < 1, phi tilde > 1). Residual plots suggest that we may have some outliers and potential over dispersion, though.

### Trying to lower AIC
```{r Optimize according to AIC}
library(MASS)
fit2 <- stepAIC(fit1, direction = 'both',trace =F)
summary(fit2)
```

```{r Compare fit1 and fit2}

anova(fit1,fit2, test = 'Chi')
```
Step AIC yielded model without ch2 (fit2), but model did not explain sigificantly more variation than fit1. AIC slightly lower in fit2. 

### Set some variables as factors

```{r}
pharmacy_data$sex <- factor(pharmacy_data$sex, labels = c('M','F'))
pharmacy_data$lp <- factor(pharmacy_data$lp, labels = c('uncovered','covered'))
pharmacy_data$fp <- factor(pharmacy_data$fp, labels = c('nonprivatePharm','privatePharm'))
pharmacy_data$fr <- factor(pharmacy_data$fr, labels = c('nonprivate','private'))
pharmacy_data$ch1 <- factor(pharmacy_data$ch1, labels = c('nonChronic1','chronicNoLim'))
pharmacy_data$ch2 <- factor(pharmacy_data$ch2, labels = c('nonChronic2','chronicLim'))

```

```{r}
fit3 <- glm(pc ~ ., data = pharmacy_data, family = 'poisson')
summary(fit3)
```

```{r}
fit3.2 <- stepAIC(fit3, direction = 'both',trace = F)
summary(fit3.2)
```

```{r}
anova(fit2,fit3.2, test = 'Chisq')
```

+ Switching variables to categorical doesn't seem to be having an effect. Try interactions

### Testing Interactions

```{r}
fit4 <- stepAIC(fit1, ~ .^2, direction = 'both', trace = F)
summary(fit4)
```
```{r}
anova(fit2,fit4, test = 'Chi')
```
Model with interaction terms explains significantly more of the variation in pc, also has a lower AIC. Much more complex, though. 

```{r}
rp <- residuals(fit4,type = 'pearson') #phi tilde numerator

phi_hat <- fit4$deviance/fit4$df.residual
phi_tilde <- sum(rp^2)/fit4$df.residual

fit4.phi <- as.data.frame(cbind('hat'=phi_hat,'tilde'=phi_tilde))
fit4.phi

#check for overdispersion using residuals
fv <- fitted(fit4)

par(mfrow = c(1,2))

plot(log(fv), rp, xlab = expression(log(hat(mu))),
     ylab = "Pearson Resids")
plot(log(fv), log(residuals(fit4, type = 'response')^2), xlab = expression(log(hat(mu))),
     ylab = expression(log(y - hat(mu))^2))
abline(0,1)
```

```{r}
library(boot)
glm.diag.plots(fit4)
```

```{r Partial Residuals}
fit4.partials <- residuals(fit4,type = 'partial')

f <- sapply(pharmacy_data, is.factor)
#par(mfrow = c(5,4))
for (iCol in colnames(pharmacy_data)){
  col_pos <- match(iCol, colnames(fit4.partials))
  if (is.na(col_pos)){
    next
  }
  
  if (f[iCol]){
    boxplot(pharmacy_data[,iCol], fit4.partials[,col_pos], ylab='partials', names=levels(pharmacy_data[,iCol]), xlab = iCol)
  } else {
    plot(pharmacy_data[,iCol],fit4.partials[,col_pos], ylab='partials', xlab = iCol)
    #lines(smooth.spline(pharmacy_data[,iCol],fit4.partials[,iCol]), col = 'red', tol = 10000, lwd = 2,
          #lty = 2)
  }
  
}
```


```{r}
plot(exp(fv), pharmacy_data[,'pc'], ylab = 'pc', xlab = 'exp(fitted values)')
lines(smooth.spline(exp(fv),pharmacy_data[,'pc']), col = 'red', lwd = 2)
#identify(exp(fv), pharmacy_data[,'pc'], labels = name)
```

### Check for variance inflation
```{r}
library(car)

#greater than 5-10 is bad
vif(fit4)
```

Clearly some issues with mulitcollinearity in our covariates. Use lasso to try and mitigate muliticollinearity and reduce the model.

```{r}
library(glmnet)

y <- pharmacy_data$pc

X <- model.matrix(fit4, data = pharmacy_data)[,-1]

lassoMod <- glmnet(X,y,family='poisson', alpha = 1, lambda.min=0, nlambda=101)

fit.lasso.cv <- cv.glmnet(X, y, lambda.min=0, nlambda=101, nfolds = nrow(pharmacy_data)) 
plot(fit.lasso.cv)

plot(lassoMod, label = T, xvar = "lambda")
text(-10,coef(lassoMod)[-1,length(lassoMod$lambda)]-0.05,labels=colnames(X),cex=1.1) 
mtext("CV estimate", side=1, at=log(fit.lasso.cv$lambda.min), cex=.8)
abline(v=log(fit.lasso.cv$lambda.min), col="red")
abline(h = 0, lty = 2, col = 'black')
```

+ Extract cross validation model

```{r}
tmp_coeffs <- coef(fit.lasso.cv, s = "lambda.min")
data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)

```

+ Create model based off cv
```{r}

fit5cv <- glm(pc ~ sex*age + ad * hs, data = pharmacy_data, family = 'poisson')
summary(fit5cv)

```

```{r}
anova(fit4, fit5cv, test = 'Cp')
```

## Create table of model diagnostics

```{r}
#useful functions
null.model <- glm(pc ~ 1, data = pharmacy_data, family='poisson')

# p-value for overall model fit
calcPval <- function(x){
  compare.null <- anova(null.model,x, test = 'Chi')
  pVal_raw <- compare.null$`Pr(>Chi)`[2]
  pVal <- formatC(pVal_raw, format = 'e', digits = 2)
  return(pVal)
}

compute_PhiTil <- function(x){
  pear_resid <- residuals(x,type='pearson')
  tilde <- sum(pear_resid^2)/x$df.residual
  return(tilde)
}

compute_PhiHat <- function(x){
  hat <- x$deviance/x$df.residual
  return(hat)
}

extract_modNum <- function(list_mods){
  nums <- list()
  for (i in 1:length(list_mods)){
    nums[i] <- deparse(substitute(list_mods[i]))
  }
  
  return(nums)
}
  
```

```{r}
models.list <- list(fit1,fit3.2,fit4,fit5cv)
models.AIC <- unlist(lapply(models.list,AIC))
models.BIC <- unlist(lapply(models.list,BIC))

model.performance.sumr <- data.frame("Model_Num" = c(1:length(models.list)),
                                     "Formula"=unlist(lapply(models.list,function(x)
                                                            Reduce(paste,deparse(formula(x))))),
                                    "Deviance"=unlist(lapply(models.list,function(x)
                                                        summary(x)$deviance)), 
                                    "pF"=unlist(lapply(models.list, calcPval)),
                                    "AIC" = models.AIC,"BIC"=models.BIC,
                                    "Phi_Tild" = unlist(lapply(models.list, compute_PhiTil)),
                                    "Phi_Hat" = unlist(lapply(models.list, compute_PhiHat)))
model.performance.sumr <- model.performance.sumr[order(model.performance.sumr$BIC),]

View(model.performance.sumr)
```

