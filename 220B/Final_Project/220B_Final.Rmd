---
title: "220B_Final_Project"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggplot2)
library(GGally)
library(dplyr)
library(ggpubr)
```


```{r directorys, include =FALSE}

#Change working directory depending if on Mac vs Windows

tryCatch({
  setwd("/Users/owner/Desktop/PSTAT")},
  error = function(e) {
    print('Path for Mac working directory not found, trying Windows...')
    setwd("D:/PSTAT/")})
```


# Project Goal:

For this project, suppose a survey was conducted within San Francisco to investigate the number of individual consultations with pharmacists.

+ Investigate how the number of consultations with a pharmacist is associated with other
variables. As part of this investigation, develop a model for pharmacy managers to estimate
the expected number of pharmacist consultations by a new customer within a 4 week period, if
the pharmacy was provided with values of the other variables for that new customer.

+ Clearly state and discuss your model(s) and assumptions, and also the limitations of your analysis in the context of this study. Within the discussion section, you may include any questions you would like to ask the people who designed the survey and collected these data

```{r Load in data}
pharmacy_data <- read.table("pharmacist.txt",header = T)

pharmacy_data$age <- pharmacy_data$age*100
pharmacy_data$income <- pharmacy_data$income*1000

#inspect first few rows
head(pharmacy_data)

```

### We are trying to predict pc, which is the number of consultations with a pharmacist in the past 4 weeks.
+ pc variable represents counts, so we are modeling a Poisson distribution

## EDA

```{r}
getMode = function(x){
    ta = table(x)
    tam = max(ta)
    if (all(ta == tam))
         mod = NA
    else
         if(is.numeric(x))
    mod = as.numeric(names(ta)[ta == tam])
    else
         mod = names(ta)[ta == tam]
    return(mod)
}

means <- apply(pharmacy_data,2,mean)
mode <- apply(pharmacy_data,2, getMode)
med <- apply(pharmacy_data,2,median)
variance <- apply(pharmacy_data,2, function(x) round(var(x),2))

as.data.frame(cbind(means,mode,med,variance))
```


```{r Kernel Densities, warning=FALSE}
library(reshape2)

long_df=melt(pharmacy_data)
kernel_plots <- ggplot(long_df, aes(value, colour = variable)) + 
  geom_density(aes(fill=variable),alpha = 0.1)+
  facet_wrap(~variable, scales = "free") + 
  labs(color = 'Subject\nResponses') + guides(fill=F) + 
  theme(text = element_text(size = 12))

kernel_plots
```

Data either follows a bimodal, exponential, or skewed normal distribution. 


```{r Box Plots}
cov.boxs <- ggplot(stack(pharmacy_data), aes(x = ind, y = values, fill = ind)) +
  geom_boxplot(outlier.color = 'red', alpha = 0.4) + facet_wrap(~ind, scales = 'free')+
  theme(legend.position = "none", axis.title = element_blank(), text = element_text(size = 15))
cov.boxs
```

```{r Correlations}
library(Hmisc)

my_fn <- function(data, mapping, pts=list(), smt=list(), ...){
  ggplot(data = data, mapping = mapping, ...) + 
    do.call(geom_point, pts) +
    do.call(geom_smooth, smt) 
}

#p-values of correlations
rcorr(as.matrix(pharmacy_data), type = c('pearson'))

all_corr <- ggcorr(pharmacy_data, geom = "tile", label = T, hjust = .8, size = 5, color = "grey50")

# only plot the interesting correlations between pc, age, ad
p1 <- ggplot(pharmacy_data, aes(x = pc, y = ad)) +
  geom_point() + geom_smooth(method = 'lm', se = F, size = 1.2, color = 'red', alpha = 0.5)+
  labs(title = 'pc vs age') + theme_classic() + theme(plot.title = element_text(hjust = 0.5))
# 
# p2 <- ggplot(pharmacy_data, aes(x = age, y = fr)) +
#   geom_point() + geom_smooth(method = 'lm', se = F, size = 1.2, color = 'green', alpha = 0.5) + 
#   labs(title = 'age vs fr') + theme_classic() +  theme(plot.title = element_text(hjust = 0.5))
# 
# p3 <- ggplot(pharmacy_data, aes(x = lp, y = fr)) +
#   geom_point() + geom_smooth(method = 'lm', se = F, size = 1.2, color = 'blue', alpha = 0.5) +
#   labs(title = 'lp vs fr') + theme_classic() +  theme(plot.title = element_text(hjust = 0.5))

int_corr <- ggarrange(all_corr,p1, nrow = 1, ncol = 2)


annotate_figure(int_corr, top =text_grob ("Correlations Between Survey Measures", face = "bold", size = 14))
```

Strongest correlations are between pc, ad, and age. These are the ones plotted. Everything else is around the +/- 0.3 range. Ad might be strongest predictor of pc

```{r Compare mean and variance to determine if negative binom a possible random component}

#if mean and variance are the same, then poisson is the possible random. If variance greater, then negative binom is random
funs <- function(x) { c(mean = mean(x), var=var(x))} 
descrips <- as.data.frame(sapply(pharmacy_data, funs))
descrips
```

Variance > mean of pc and ad, suggesting that the negative binomial might be the best option.


# Being Modeling

## Trying out a poisson model
```{r First pass}
fit1 <- glm(pc ~ ., family = 'poisson', data = pharmacy_data)

summary(fit1)

#check phi for over dispersion
rp <- residuals(fit1,type = 'pearson') #phi tilde numerator

phi_hat <- fit1$deviance/fit1$df.residual
phi_tilde <- sum(rp^2)/fit1$df.residual

fit1.phi <- as.data.frame(cbind('hat'=phi_hat,'tilde'=phi_tilde))
fit1.phi

#check for overdispersion using residuals
fv <- fitted(fit1)

par(mfrow = c(1,2))

plot(log(fv), rp, xlab = expression(log(hat(mu))),
     ylab = "Pearson Resids")
plot(log(fv), log(residuals(fit1, type = 'response')^2), xlab = expression(log(hat(mu))),
     ylab = expression(log(y - hat(mu))^2))
abline(0,1)
```
First pass of the model indicates that sex, age, ill, ad, hs, and ch1 are all significant predictors. Deviance diagnostics suggest that poisson is the true random component (phi hat < 1, phi tilde > 1). Residual plots suggest that we may have some outliers and potential over dispersion, though.

### Trying to lower AIC
```{r Optimize according to AIC}
library(MASS)
fit2 <- stepAIC(fit1, direction = 'both',trace =F)
summary(fit2)
```

```{r Compare fit1 and fit2}

anova(fit1,fit2, test = 'Chi')
```
Step AIC yielded model without ch2 (fit2), but model did not explain sigificantly more variation than fit1. AIC slightly lower in fit2. 

### Set some variables as factors

```{r Convert to Factors}
pharmacy_data$sex <- factor(pharmacy_data$sex, labels = c('M','F'))
pharmacy_data$lp <- factor(pharmacy_data$lp, labels = c('uncovered','covered'))
pharmacy_data$fp <- factor(pharmacy_data$fp, labels = c('nonprivatePharm','privatePharm'))
pharmacy_data$fr <- factor(pharmacy_data$fr, labels = c('nonprivate','private'))
pharmacy_data$ch1 <- factor(pharmacy_data$ch1, labels = c('nonChronic1','chronicNoLim'))
pharmacy_data$ch2 <- factor(pharmacy_data$ch2, labels = c('nonChronic2','chronicLim'))

```

```{r}
fit3 <- glm(pc ~ ., data = pharmacy_data, family = 'poisson')
summary(fit3)
```

```{r}
fit3.2 <- stepAIC(fit3, direction = 'both',trace = F)
summary(fit3.2)
```

```{r}
anova(fit2,fit3.2, test = 'Chisq')
```

+ Switching variables to categorical doesn't seem to be having an effect. Try interactions

### Testing Interactions

```{r}
fit4 <- stepAIC(fit1, ~ .^2, direction = 'both', trace = F)
summary(fit4)
```

```{r}
anova(fit2,fit4, test = 'Chi')
```
Model with interaction terms explains significantly more of the variation in pc, also has a lower AIC. Much more complex, though. 

```{r}
rp <- residuals(fit4,type = 'pearson') #phi tilde numerator

phi_hat <- fit4$deviance/fit4$df.residual
phi_tilde <- sum(rp^2)/fit4$df.residual

fit4.phi <- as.data.frame(cbind('hat'=phi_hat,'tilde'=phi_tilde))
fit4.phi

#check for overdispersion using residuals
fv <- fitted(fit4)

par(mfrow = c(1,2))

plot(log(fv), rp, xlab = expression(log(hat(mu))),
     ylab = "Pearson Resids")
plot(log(fv), log(residuals(fit4, type = 'response')^2), xlab = expression(log(hat(mu))),
     ylab = expression(log(y - hat(mu))^2))
abline(0,1)
```

```{r}
library(boot)
glm.diag.plots(fit4)
```

```{r Partial Residuals}
fit4.partials <- residuals(fit4,type = 'partial')

f <- sapply(pharmacy_data, is.factor)
#par(mfrow = c(5,4))
for (iCol in colnames(pharmacy_data)){
  col_pos <- match(iCol, colnames(fit4.partials))
  if (is.na(col_pos)){
    next
  }
  
  if (f[iCol]){
    boxplot(pharmacy_data[,iCol], fit4.partials[,col_pos], ylab='partials', names=levels(pharmacy_data[,iCol]), xlab = iCol)
  } else {
    plot(pharmacy_data[,iCol],fit4.partials[,col_pos], ylab='partials', xlab = iCol)
    #lines(smooth.spline(pharmacy_data[,iCol],fit4.partials[,iCol]), col = 'red', tol = 10000, lwd = 2,
          #lty = 2)
  }
  
}
```


```{r}
plot(exp(fv), pharmacy_data[,'pc'], ylab = 'pc', xlab = 'exp(fitted values)')
lines(smooth.spline(exp(fv),pharmacy_data[,'pc']), col = 'red', lwd = 2)
#identify(exp(fv), pharmacy_data[,'pc'], labels = name)
```

### Check for variance inflation
```{r}
library(car)

#greater than 5-10 is bad
vif(fit4)
```

Clearly some issues with mulitcollinearity in our covariates. Use lasso to try and mitigate muliticollinearity and reduce the model.

```{r}
library(glmnet)

y <- pharmacy_data$pc

X <- model.matrix(fit4, data = pharmacy_data)[,-1]

lassoMod <- glmnet(X,y,family='poisson', alpha = 1, lambda.min=0, nlambda=101)

fit.lasso.cv <- cv.glmnet(X, y, lambda.min=0, nlambda=101, nfolds = nrow(pharmacy_data)) 
plot(fit.lasso.cv)

plot(lassoMod, label = T, xvar = "lambda")
text(-10,coef(lassoMod)[-1,length(lassoMod$lambda)]-0.05,labels=colnames(X),cex=1.1) 
mtext("CV estimate", side=1, at=log(fit.lasso.cv$lambda.min), cex=.8)
abline(v=log(fit.lasso.cv$lambda.min), col="red")
abline(h = 0, lty = 2, col = 'black')
```

+ Extract cross validation model

```{r}
tmp_coeffs <- coef(fit.lasso.cv, s = "lambda.min")
data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)

```

+ Create model based off cv
```{r}

fit5cv <- glm(pc ~ sex*age + ad * hs, data = pharmacy_data, family = 'poisson')
summary(fit5cv)

```

```{r}
anova(fit4, fit5cv, test = 'Cp')
```

## Create table of model diagnostics

```{r}
#useful functions
null.model <- glm(pc ~ 1, data = pharmacy_data, family='poisson')

# p-value for overall model fit
calcPval <- function(x){
  compare.null <- anova(null.model,x, test = 'Chi')
  pVal_raw <- compare.null$`Pr(>Chi)`[2]
  pVal <- formatC(pVal_raw, format = 'e', digits = 2)
  return(pVal)
}

compute_PhiTil <- function(x){
  pear_resid <- residuals(x,type='pearson')
  tilde <- sum(pear_resid^2)/x$df.residual
  return(tilde)
}

compute_PhiHat <- function(x){
  hat <- x$deviance/x$df.residual
  return(hat)
}

extract_modNum <- function(list_mods){
  nums <- list()
  for (i in 1:length(list_mods)){
    nums[i] <- deparse(substitute(list_mods[i]))
  }
  
  return(nums)
}
  
```

```{r}
models.list <- list(fit1,fit3.2,fit4,fit5cv)
models.AIC <- unlist(lapply(models.list,AIC))
models.BIC <- unlist(lapply(models.list,BIC))

#R^2 is not the same here as it is in the gaussian case. Instead of explaining the amount of variation,
#it represents the ratio of how close the fit is to being either the perfect or worst model. 
#SMALLER DEVIANCE MEANS CLOSER TO PERFECT MODEL, thus ratios closer to 1 mean better models
models.R2 <- unlist(lapply(models.list, function(x) 1-(x$deviance/x$null.deviance)))

model.performance.sumr <- data.frame("Model_Num" = c(1:length(models.list)),
                                     "Model_Name" = c("fit1","fit3.2","fit4","fit5cv"),
                                     "Formula"=unlist(lapply(models.list,function(x)
                                                            Reduce(paste,deparse(formula(x))))),
                                    "Deviance"=unlist(lapply(models.list,function(x)
                                                        summary(x)$deviance)),
                                    "R^2"=models.R2,
                                    "pF"=unlist(lapply(models.list, calcPval)),
                                    "AIC" = models.AIC,"BIC"=models.BIC,
                                    "Phi_Tild" = unlist(lapply(models.list, compute_PhiTil)),
                                    "Phi_Hat" = unlist(lapply(models.list, compute_PhiHat)))
model.performance.sumr <- model.performance.sumr[order(model.performance.sumr$BIC),]
```


```{r}
anova(fit3.2,fit4, test = 'Chi')
```


## Choosing pc = sex + age + fp + fr + ill + ad + hs + ch1 based off of BIC, plus multicollinearity is and issue with fit4

```{r}
#Final diagnostics

vif(fit3.2) #multicollinearity a non issue

par(mfrow = c(2,2))
plot(fit3.2)
```

### Remove high leverage/cooks distance points
```{r}
fit3.3 <- update(fit3.2, subset = -c(9,292,430,400))
```

+ Removal of potentially high residual influencers dramatically drops BIC. R^2 ratio barely increases

```{r}
# see if we can simplify further
summary(fit3.4 <- step(fit3.3,trace = F, direction = 'both', k = log(nobs(fit3.3))))
anova(fit3.4,fit3.3, test = 'Chi')
```

Anova results indicate that fit3.4 does not have significantly less deviance than more complex 3.3 model. Thus,
retain 3.3

```{r}
termplot(fit3.4, data = pharmacy_data, partial.resid = T, col.res = 'black', cex = 1, pch = 1)
abline(h=0,lty=2, col='gray', lwd = 2)
```

```{r}
fv <- fitted(fit3.4)
#response residuals are y-fitted(y)
plot(fv, residuals(fit3.4, type = 'response'), xlab = expression(log(hat(mu))),
     ylab = expression(log(y - hat(mu))^2))
lines(smooth.spline(fv,residuals(fit3.4, type = 'response')), col = 'red', lwd = 2)
```

## Test negative binomial model

```{r}
summary(fitnb <- glm.nb(pc ~ ., data = pharmacy_data))
```

```{r}
summary(fitnb.2 <- step(fitnb, trace = F, direction='both', k = log(nobs(fitnb))))
```

```{r}
summary(fitnb.3 <- update(fitnb.2, subset = -c(9,292,22)))
```

```{r}
vif(fitnb.3)
```

```{r}
fv.nb <- fitted(fitnb.3)
plot(exp(fv.nb), exp(residuals(fitnb.3, type = 'response')), xlab = expression(log(hat(mu))),
     ylab = expression(log(y - hat(mu))^2))
lines(smooth.spline(exp(fv.nb),exp(residuals(fitnb.3, type = 'response'))), col = 'red', lwd = 2)
```

+ Need to check if categorical variables are significant, anova comparison between model w/ vs w/o
  fp is significant


```{r}
new.models <- list(fit3.3,fit3.4, fitnb.3)
new.models.BIC <- unlist(lapply(new.models,BIC))
new.models.AIC <- unlist(lapply(new.models,AIC))
new.models.R2 <- unlist(lapply(new.models, function(x) 1-(x$deviance/x$null.deviance)))

new.models.df <- data.frame("Model_Num" = c(max(model.performance.sumr$Model_Num)+1:length(new.models)),
                            "Type" = c('poisson','poisson','nB'),
                            "Model_Name" = c("fit3.3","fit3.4","fitnb.3"),
                                     "Formula"=unlist(lapply(new.models,function(x)
                                                            Reduce(paste,deparse(formula(x))))),
                                    "Deviance"=unlist(lapply(new.models,function(x)
                                                        summary(x)$deviance)),
                                    "R^2"=new.models.R2,
                                    "AIC" = new.models.AIC,"BIC"=new.models.BIC,
                                    "Phi_Tild" = unlist(lapply(new.models, compute_PhiTil)),
                                    "Phi_Hat" = unlist(lapply(new.models, compute_PhiHat)))

#drop p values comparing to null model, not very helpful here
model.performance.sumr <- model.performance.sumr[,!names(model.performance.sumr) %in% c('pF')]
model.performance.sumr$Type <- rep('poisson',nrow(model.performance.sumr))

model.performance.sumr <- rbind(model.performance.sumr,new.models.df)

model.performance.sumr <- model.performance.sumr[with(model.performance.sumr,order(-R.2,model.performance.sumr$BIC)),]

model.performance.sumr <- model.performance.sumr[,c('Model_Num','Model_Name','Type','Formula','Deviance',
                                                    'R.2','AIC','BIC','Phi_Tild','Phi_Hat')]

View(model.performance.sumr)
```

# Simulations
```{r Poisson Simulation}
n <- 400

# pc = sex + age + fp + fr + ill + ad + hs + ch1
de.factorDF <- pharmacy_data[-1] %>% mutate_if(is.factor,as.numeric)

de.factorDF <- de.factorDF[-c(9,292,430,400),]

X <- as.data.frame(apply(de.factorDF,2,function(x) sample(min(x):max(x),n, replace = T)))

betas <- coef(fit3.4)[-1]

#mu <- exp(coef(fit3.3)[1] + rowSums(sweep(X[,names(fit3.4$model)[-1]], MARGIN = 2, betas, '*')))

X[,c('sex','fp','fr','ch1','ch2')] <- apply(X[,c('sex','fp','fr','ch1','ch2')], 2, function(x) x-1)

mu <- exp(coef(fit3.4)[1] + betas[1]*X[,'sex'] + betas[2]*X[,'fp'] + betas[3]*X[,'ill'] +
            betas[4]*X[,'ad'] + betas[5]*X[,'hs'])

#create to simulate nb later
mu.nb <- exp(coef(fitnb.3)[1] + coef(fitnb.3)[2]*X[,'sex'] + coef(fitnb.3)[3]*X[,'fp']+
               coef(fitnb.3)[4]*X[,'ill'] + coef(fitnb.3)[5]*X[,'ad'])

set.seed(23)
y <- rpois(n, lambda=mu)

X$sex <- factor(X$sex, labels = c('M','F'))
X$lp <- factor(X$lp, labels = c('uncovered','covered'))
X$fp <- factor(X$fp, labels = c('nonprivatePharm','privatePharm'))
X$fr <- factor(X$fr, labels = c('nonprivate','private'))
X$ch1 <- factor(X$ch1, labels = c('nonChronic1','chronicNoLim'))
X$ch2 <- factor(X$ch2, labels = c('nonChronic2','chronicLim'))

summary(sim <- glm(y ~ ., data = X[,names(fit3.4$model)[-1]], family = poisson(link=log)))


sim.fv <- fitted(sim)

par(mfrow=c(2,3))
plot(sim,which=c(1:6))

par(mfrow=c(2,4))
termplot(sim, partial.resid=T, col.res = 'black')
```


```{r NB simulation}
set.seed(123)
y.nb <- rnegbin(n,mu.nb,theta = fitnb.3$theta)

summary( sim.nb <- glm.nb(y.nb ~ ., data = X[,names(fitnb.3$model)[-1]]))

par(mfrow = c(2,3))
plot(sim.nb, which = c(1:6))

par(mfrow=c(2,4))
termplot(sim.nb, partial.resid=T, col.res = 'black')
```

# Predictions

+ For the first person in this dataset, use your fitted model to estimate the probability distribution for his/her number of pharmacy consultations within a 4 week period, i.e., estimate the probability that his/her number of pharmacist consultations equals 0,1,2, etc.

```{r Prediction Point Estimates}
pos.pred <- predict.glm(fit3.4,pharmacy_data[1,-1], type = 'response', se.fit = T)
nb.pred <- predict.glm(fitnb.3, pharmacy_data[1,-1], type = 'response', se.fit = T)

as.data.frame(cbind('pois' = unlist(pos.pred), 'nb'=unlist(nb.pred)))
```

```{r Prediction Probability distributions}
set.seed(43)

#get quantiles for plotting
possiblities <- c(0:3)
prob.pois <- dpois(possiblities, lambda = pos.pred$fit)
prob.nb <- dnbinom(possiblities, size = fitnb.3$theta, mu = nb.pred$fit)

prob.df <- as.data.frame(cbind(possiblities,prob.pois))

prob.nb.df <- as.data.frame(cbind(possiblities,prob.nb))


pois.prob.gg <- ggplot(prob.df, aes(x=factor(possiblities), y = prob.pois, group = 1)) +
  geom_point(color= 'blue', size = 3, pch = 1) + 
  geom_col(color = 'black',fill = 'turquoise',alpha = 0.4) +
  ylim(0,1) +
  labs(x = 'Pharmacy Consultations (in 4 week period)', 
       y = 'Estimated Probability', title = 'Estimated Probability Distribution\nFor First Person',
       subtitle = '(Model 2: Poiss.)') +
  theme_bw() + theme(plot.title = element_text(hjust = 0.5, size = 14), 
                     plot.subtitle = element_text(hjust = 0.5, color = 'red', face = 'italic'))

nb.prob.gg <- ggplot(prob.nb.df, aes(x=factor(possiblities), y = prob.nb, group = 1)) +
  geom_point(color= 'black', size = 3, pch = 1) + 
  geom_col(color = 'black',fill = 'green',alpha = 0.4) +
  ylim(0,1) +
  labs(x = 'Pharmacy Consultations (in 4 week period)', 
       y = 'Estimated Probability', title = 'Estimated Probability Distribution\nFor First Person',
       subtitle = '(Model 4: Negative Bin.)') +
  theme_bw() + theme(plot.title = element_text(hjust = 0.5, size = 14), 
                     plot.subtitle = element_text(hjust = 0.5, color = 'red', face = 'italic'))

ggarrange(pois.prob.gg,nb.prob.gg)
  
```

### Prediction slightly better for poisson model. Going with poisson!

+ fit3.4. note that fp is nonsignificant, but removing it significantly increases the deviance


```{r Test Quassi-Poisson for shits and giggle}
summary(fit.qp <- glm(pc ~ ., data = pharmacy_data, family = 'quasipoisson'))
```

```{r}
summary(fit.qp2 <- update(fit.qp, . ~ . -ch2 -fp -income -lp))
```

```{r}
anova(fit.qp2,fit.qp, test = 'Chi')
```

```{r}
fit.qp3 <- update(fit.qp2, subset = -c(9,292,400))

par(mfrow=c(2,3))
plot(fit.qp3, which = c(1:6))

par(mfrow=c(2,4))
termplot(fit.qp3, partial.resid=T, col.res = 'black')
```

No added benefits from quasi-poisson model


```{r}
par(mfrow=c(2,2))
plot(fit3.2, which=c(1,5))
plot(fitnb.2, which = c(1,5))
```


```{r}
for (i in c(2:6)){
  for (j in c(2:6)){
    if (i != j){
      print(cat(paste(names(coef(fit3.4))[i],'vs',names(coef(fit3.4)[j]),'\n')))
      print(wald.test(vcov(fit3.4), coef(fit3.4), Terms = c(i,j)))
    }
  }
}
```

